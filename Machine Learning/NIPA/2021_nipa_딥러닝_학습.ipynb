{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021 nipa 딥러닝 학습.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIsC5XeSnT3M2hOuipw2XC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eunjung-Cho/python_study/blob/master/Machine%20Learning/NIPA/2021_nipa_%EB%94%A5%EB%9F%AC%EB%8B%9D_%ED%95%99%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2021 nipa 딥러닝 학습"
      ],
      "metadata": {
        "id": "ajFykc7CTEV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 퍼셉트론 작동 예시 구현하기"
      ],
      "metadata": {
        "id": "Q7dXXIOgTImz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 여부를 예측하는 퍼셉트론 함수\n",
        "def Perceptron(x_1,x_2):\n",
        "    \n",
        "    # 설정한 가중치값을 적용\n",
        "    w_0 = -5 \n",
        "    w_1 = -1\n",
        "    w_2 = 5\n",
        "    \n",
        "    # 활성화 함수에 들어갈 값을 계산\n",
        "    output = w_0+w_1*x_1+w_2*x_2\n",
        "    \n",
        "    # 활성화 함수 결과를 계산\n",
        "    if output < 0:\n",
        "        y = 0\n",
        "    else:\n",
        "        y = 1\n",
        "    \n",
        "    return y, output\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "1. perceptron의 예측 결과가 학습한다:1 이 나오도록\n",
        "   x_1, x_2에 적절한 값을 입력하세요.\n",
        "\"\"\"\n",
        "x_1 = 2\n",
        "x_2 = 4\n",
        "\n",
        "result, go_out = Perceptron(x_1,x_2)\n",
        "\n",
        "print(\"신호의 총합 : %d\" % go_out)\n",
        "\n",
        "if go_out > 0:\n",
        "    print(\"학습 여부 : %d\\n ==> 학습한다!\" % result)\n",
        "else:\n",
        "    print(\"학습 여부 : %d\\n ==> 학습하지 않는다!\" % result)\n"
      ],
      "metadata": {
        "id": "CNsumFVMTM4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DIY 퍼셉트론 만들기"
      ],
      "metadata": {
        "id": "VUs-e7rhTN3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. 신호의 총합과 그에 따른 결과 0 또는 1을\n",
        "   반환하는 함수 perceptron을 완성합니다.\n",
        "   \n",
        "   Step01. 입력 받은 값을 이용하여\n",
        "           신호의 총합을 구합니다.\n",
        "           \n",
        "   Step02. 신호의 총합이 0 이상이면 1을, \n",
        "           그렇지 않으면 0을 반환하는 활성화 \n",
        "           함수를 작성합니다.\n",
        "'''\n",
        "def perceptron(w, x):\n",
        "    \n",
        "    output = w[0]+w[1]*x[0]+w[2]*x[1]+w[3]*x[2]+w[4]*x[3]\n",
        "    \n",
        "    if output >= 0:\n",
        "        y=1\n",
        "    else:\n",
        "        y = 0\n",
        "    \n",
        "    return y, output\n",
        "\n",
        "# x_1, x_2, x_3, x_4의 값을 순서대로 list 형태로 저장\n",
        "x = [1,2,3,4]\n",
        "\n",
        "# w_0, w_1, w_2, w_3, w_4의 값을 순서대로 list 형태로 저장\n",
        "w = [2, -1, 1, 3, -2]\n",
        "\n",
        "# 퍼셉트론의 결과를 출력\n",
        "y, output = perceptron(w,x)\n",
        "\n",
        "print('output: ', output)\n",
        "print('y: ', y)"
      ],
      "metadata": {
        "id": "81tXsu0YTRNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 퍼셉트론의 알맞은 가중치 찾기"
      ],
      "metadata": {
        "id": "3240uEZ9UQjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def perceptron(w, x):\n",
        "    \n",
        "    output = w[1] * x[0] + w[2] * x[1] + w[0]\n",
        "    \n",
        "    if output >= 0:\n",
        "        y = 1\n",
        "    else:\n",
        "        y = 0\n",
        "    \n",
        "    return y\n",
        "\n",
        "\n",
        "\n",
        "# Input 데이터\n",
        "X = [[0,0], [0,1], [1,0], [1,1]]\n",
        "\n",
        "'''\n",
        "1. perceptron 함수의 입력으로 들어갈 가중치 값을 입력해주세요.\n",
        "   순서대로 w_0, w_1, w_2에 해당됩니다.\n",
        "'''\n",
        "w = [-2, 1, 1]\n",
        "\n",
        "# AND Gate를 만족하는지 출력하여 확인\n",
        "print('perceptron 출력')\n",
        "\n",
        "for x in X:\n",
        "    print('Input: ',x[0], x[1], ', Output: ',perceptron(w, x))\n",
        "\n"
      ],
      "metadata": {
        "id": "sHK3CCCDUTkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텐서플로우를 활용하여 신경망 구현하기 - 데이터 전 처리"
      ],
      "metadata": {
        "id": "YCwgrYzNUb2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "np.random.seed(100)\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# 데이터를 DataFrame 형태로 불러 옵니다.\n",
        "df = pd.read_csv(\"data/Advertising.csv\")\n",
        "\n",
        "# DataFrame 데이터 샘플 5개를 출력합니다.\n",
        "print('원본 데이터 샘플 :')\n",
        "print(df.head(),'\\n')\n",
        "\n",
        "# 의미없는 변수는 삭제합니다.\n",
        "df = df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "\"\"\"\n",
        "1. Sales 변수는 label 데이터로 Y에 저장하고 나머진 X에 저장합니다.\n",
        "\"\"\"\n",
        "X = df.drop(columns=['Sales'])\n",
        "Y = df['Sales']\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.3)\n",
        "\n",
        "\"\"\"\n",
        "2. 학습용 데이터를 tf.data.Dataset 형태로 변환합니다.\n",
        "   from_tensor_slices 함수를 사용하여 변환하고 batch를 수행하게 합니다.\n",
        "\"\"\"\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_X.values, train_Y.values))\n",
        "train_ds = train_ds.shuffle(len(train_X)).batch(batch_size=5)\n",
        "\n",
        "# 하나의 batch를 뽑아서 feature와 label로 분리합니다.\n",
        "[(train_features_batch, label_batch)] = train_ds.take(1)\n",
        "\n",
        "# batch 데이터를 출력합니다.\n",
        "print('\\nFB, TV, Newspaper batch 데이터:\\n',train_features_batch)\n",
        "print('Sales batch 데이터:',label_batch)"
      ],
      "metadata": {
        "id": "sHKE-FiPUqNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텐서플로우를 활용하여 신경망 구현하기 - 모델 구현"
      ],
      "metadata": {
        "id": "jrNnBgALVCl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "np.random.seed(100)\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# 데이터를 DataFrame 형태로 불러 옵니다.\n",
        "df = pd.read_csv(\"data/Advertising.csv\")\n",
        "\n",
        "# DataFrame 데이터 샘플 5개를 출력합니다.\n",
        "print('원본 데이터 샘플 :')\n",
        "print(df.head(),'\\n')\n",
        "\n",
        "# 의미없는 변수는 삭제합니다.\n",
        "df = df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "X = df.drop(columns=['Sales'])\n",
        "Y = df['Sales']\n",
        "\n",
        "# 학습용 테스트용 데이터로 분리합니다.\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.3)\n",
        "\n",
        "# Dataset 형태로 변환합니다.\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_X.values, train_Y))\n",
        "train_ds = train_ds.shuffle(len(train_X)).batch(batch_size=5)\n",
        "\n",
        "\"\"\"\n",
        "1. tf.keras.models.Sequential()를 활용하여 신경망 모델을 생성합니다.\n",
        "   자유롭게 layers를 쌓고 마지막 layers는 노드 수를 1개로 설정합니다.\n",
        "\"\"\"\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(10, input_shape=(3, ) ),\n",
        "    tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "ueEuwGAkVEil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텐서플로우를 활용하여 신경망 구현하기 - 모델 학습"
      ],
      "metadata": {
        "id": "Y9Um2AxpVaVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "np.random.seed(100)\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# 데이터를 DataFrame 형태로 불러 옵니다.\n",
        "df = pd.read_csv(\"data/Advertising.csv\")\n",
        "\n",
        "# DataFrame 데이터 샘플 5개를 출력합니다.\n",
        "print('원본 데이터 샘플 :')\n",
        "print(df.head(),'\\n')\n",
        "\n",
        "# 의미없는 변수는 삭제합니다.\n",
        "df = df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "X = df.drop(columns=['Sales'])\n",
        "Y = df['Sales']\n",
        "\n",
        "# 학습용 테스트용 데이터로 분리합니다.\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.3)\n",
        "\n",
        "# Dataset 형태로 변환합니다.\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_X.values, train_Y))\n",
        "train_ds = train_ds.shuffle(len(train_X)).batch(batch_size=5)\n",
        "\n",
        "\n",
        "# keras를 활용하여 신경망 모델을 생성합니다.\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(10, input_shape=(3,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "1. 학습용 데이터를 바탕으로 모델의 학습을 수행합니다.\n",
        "    \n",
        "step1. compile 메서드를 사용하여 최적화 모델 설정합니다.\n",
        "       loss는 mean_squared_error, optimizer는 adam으로 설정합니다.\n",
        "       \n",
        "step2. fit 메서드를 사용하여 Dataset으로 변환된 학습용 데이터를 학습합니다.\n",
        "       epochs는 100으로 설정합니다.\n",
        "\"\"\"\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "history = model.fit(train_ds, epochs=100, verbose=2)"
      ],
      "metadata": {
        "id": "CpPWeblmVveU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텐서플로우를 활용하여 신경망 구현하기 - 모델 평가 및 예측"
      ],
      "metadata": {
        "id": "aDhOTvTaVyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "np.random.seed(100)\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# 데이터를 DataFrame 형태로 불러 옵니다.\n",
        "df = pd.read_csv(\"data/Advertising.csv\")\n",
        "\n",
        "# DataFrame 데이터 샘플 5개를 출력합니다.\n",
        "print('원본 데이터 샘플 :')\n",
        "print(df.head(),'\\n')\n",
        "\n",
        "# 의미없는 변수는 삭제합니다.\n",
        "df = df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "X = df.drop(columns=['Sales'])\n",
        "Y = df['Sales']\n",
        "\n",
        "# 학습용 테스트용 데이터로 분리합니다.\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.3)\n",
        "\n",
        "# Dataset 형태로 변환합니다.\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_X.values, train_Y))\n",
        "train_ds = train_ds.shuffle(len(train_X)).batch(batch_size=5)\n",
        "\n",
        "# keras를 활용하여 신경망 모델을 생성합니다.\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(10, input_shape=(3,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "# 학습용 데이터를 바탕으로 모델의 학습을 수행합니다.\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "history = model.fit(train_ds, epochs=100, verbose=2)\n",
        "\n",
        "\"\"\"\n",
        "1. evaluate 메서드를 사용하여 테스트용 데이터의 loss 값을 계산합니다.\n",
        "\"\"\"\n",
        "loss = model.evaluate(test_X, test_Y, verbose=0)\n",
        "\n",
        "\"\"\"\n",
        "2. predict 메서드를 사용하여 테스트용 데이터의 예측값을 계산합니다.\n",
        "\"\"\"\n",
        "predictions = model.predict(test_X)\n",
        "\n",
        "# 결과를 출력합니다.\n",
        "print(\"테스트 데이터의 Loss 값: \", loss)\n",
        "for i in range(5):\n",
        "    print(\"%d 번째 테스트 데이터의 실제값: %f\" % (i, test_Y.iloc[i]))\n",
        "    print(\"%d 번째 테스트 데이터의 예측값: %f\" % (i, predictions[i][0]))\n"
      ],
      "metadata": {
        "id": "gdGuD9NPV9Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 신경망 모델로 분류하기"
      ],
      "metadata": {
        "id": "SwXMqRBmWT24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "np.random.seed(100)\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# sklearn에 저장된 데이터를 불러 옵니다.\n",
        "X, Y = load_iris(return_X_y = True)\n",
        "\n",
        "# DataFrame으로 변환\n",
        "df = pd.DataFrame(X, columns=['꽃받침 길이','꽃받침 넓이', '꽃잎 길이', '꽃잎 넓이'])\n",
        "df['클래스'] = Y\n",
        "\n",
        "X = df.drop(columns=['클래스'])\n",
        "Y = df['클래스']\n",
        "\n",
        "# 학습용 평가용 데이터로 분리합니다\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state = 42)\n",
        "\n",
        "# Dataset 형태로 변환합니다.\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_X.values, train_Y))\n",
        "train_ds = train_ds.shuffle(len(train_X)).batch(batch_size=5)\n",
        "\n",
        "\"\"\"\n",
        "1. keras를 활용하여 신경망 모델을 생성합니다.\n",
        "   3가지 범주를 갖는 label 데이터를 분류하기 위해서 마지막 레이어 노드를 아래와 같이 설정합니다.\n",
        "\"\"\"\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(10, input_dim=4),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "# 학습용 데이터를 바탕으로 모델의 학습을 수행합니다.\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(train_ds, epochs=100, verbose=2)\n",
        "\n",
        "# 테스트용 데이터를 바탕으로 학습된 모델을 평가합니다.\n",
        "loss, acc = model.evaluate(test_X, test_Y)\n",
        "\n",
        "# 테스트용 데이터의 예측값을 구합니다.\n",
        "predictions = model.predict(test_X)\n",
        "\n",
        "# 결과를 출력합니다.\n",
        "print(\"테스트 데이터의 Accuracy 값: \", acc)\n",
        "for i in range(5):\n",
        "    print(\"%d 번째 테스트 데이터의 실제값: %d\" % (i, test_Y.iloc[i]))\n",
        "    print(\"%d 번째 테스트 데이터의 예측값: %d\" % (i, np.argmax(predictions[i])))"
      ],
      "metadata": {
        "id": "OvqhSECDV-fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 다양한 신경망"
      ],
      "metadata": {
        "id": "g0wa0mQ5Wepu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST 분류 CNN 모델 - 데이터 전 처리"
      ],
      "metadata": {
        "id": "KUG3bhrsWc86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from elice_utils import EliceUtils\n",
        "\n",
        "elice_utils = EliceUtils()\n",
        "\n",
        "import logging, os\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "# 동일한 실행 결과 확인을 위한 코드입니다.\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "\n",
        "# MNIST 데이터 세트를 불러옵니다.\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "# MNIST 데이터 세트를 Train set과 Test set으로 나누어 줍니다.\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()    \n",
        "\n",
        "# Train 데이터 5000개와 Test 데이터 1000개를 사용합니다.\n",
        "train_images, train_labels = train_images[:5000], train_labels[:5000]\n",
        "test_images, test_labels = test_images[:1000], test_labels[:1000]\n",
        "\n",
        "\n",
        "print(\"원본 학습용 이미지 데이터 형태: \",train_images.shape)\n",
        "print(\"원본 평가용 이미지 데이터 형태: \",test_images.shape)\n",
        "print(\"원본 학습용 label 데이터: \",train_labels)\n",
        "\n",
        "# 첫 번째 샘플 데이터를 출력합니다.\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(train_images[0], cmap=plt.cm.binary)\n",
        "plt.colorbar()\n",
        "plt.title(\"Training Data Sample\")\n",
        "plt.savefig(\"sample1.png\")\n",
        "elice_utils.send_image(\"sample1.png\")\n",
        "\n",
        "# 9개의 학습용 샘플 데이터를 출력합니다.\n",
        "class_names = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.savefig(\"sample2.png\")\n",
        "elice_utils.send_image(\"sample2.png\")\n",
        "\n",
        "\"\"\"\n",
        "1. CNN 모델의 입력으로 사용할 수 있도록 (샘플개수, 가로픽셀, 세로픽셀, 1) 형태로 변환합니다.\n",
        "\"\"\"\n",
        "train_images = tf.expand_dims(train_images, -1)\n",
        "test_images = tf.expand_dims(train_images, -1)\n",
        "\n",
        "print(\"변환한 학습용 이미지 데이터 형태: \",train_images.shape)\n",
        "print(\"변환한 평가용 이미지 데이터 형태: \",test_images.shape)"
      ],
      "metadata": {
        "id": "LXi3mJ6fWvY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST 분류 CNN 모델 - 모델 구현"
      ],
      "metadata": {
        "id": "eRY-vE5GWxgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from visual import *\n",
        "from elice_utils import EliceUtils\n",
        "\n",
        "elice_utils = EliceUtils()\n",
        "\n",
        "import logging, os\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "# 동일한 실행 결과 확인을 위한 코드입니다.\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "\n",
        "# MNIST 데이터 세트를 불러옵니다.\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "# MNIST 데이터 세트를 Train set과 Test set으로 나누어 줍니다.\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()    \n",
        "\n",
        "# Train 데이터 5000개와 Test 데이터 1000개를 사용합니다.\n",
        "train_images, train_labels = train_images[:5000], train_labels[:5000]\n",
        "test_images, test_labels = test_images[:1000], test_labels[:1000]\n",
        "\n",
        "# CNN 모델의 입력으로 사용할 수 있도록 (샘플개수, 가로픽셀, 세로픽셀, 1) 형태로 변환합니다.\n",
        "train_images = tf.expand_dims(train_images, -1)\n",
        "test_images = tf.expand_dims(test_images, -1)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "1. CNN 모델을 설정합니다.\n",
        "   분류 모델에 맞게 마지막 레이어의 노드 수는 10개, activation 함수는 'softmax'로 설정합니다.\n",
        "\"\"\"\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME', input_shape = (28,28,1)),\n",
        "    tf.keras.layers.MaxPool2D(padding = 'SAME'),\n",
        "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'),\n",
        "    tf.keras.layers.MaxPool2D(padding = 'SAME'),\n",
        "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'),\n",
        "    tf.keras.layers.MaxPool2D(padding = 'SAME'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "])\n",
        "\n",
        "# CNN 모델 구조를 출력합니다.\n",
        "print(model.summary())\n",
        "\n",
        "# CNN 모델의 학습 방법을 설정합니다.\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])\n",
        "              \n",
        "# 학습을 수행합니다. \n",
        "history = model.fit(train_images, train_labels, epochs = 20, batch_size = 512)\n",
        "\n",
        "# 학습 결과를 출력합니다.\n",
        "Visulaize([('CNN', history)], 'loss')"
      ],
      "metadata": {
        "id": "w8OD-wlbW0GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST 분류 CNN 모델 - 평가 및 예측\n",
        "\n",
        "\n",
        "Keras에서 CNN 모델의 평가 및 예측을 위해 필요한 함수/메서드\n",
        "\n",
        "평가 방법\n",
        "\n",
        "model.evaluate(X, Y)\n",
        "Copy\n",
        "evaluate() 메서드는 학습된 모델을 바탕으로 입력한 feature 데이터 X와 label Y의 loss 값과 metrics 값을 출력합니다.\n",
        "\n",
        "예측 방법\n",
        "\n",
        "model.predict_classes(X)\n",
        "Copy\n",
        "X 데이터의 예측 label 값을 출력합니다."
      ],
      "metadata": {
        "id": "d3uAnUYQX26c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from visual import *\n",
        "from plotter import *\n",
        "from elice_utils import EliceUtils\n",
        "\n",
        "elice_utils = EliceUtils()\n",
        "\n",
        "import logging, os\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "# 동일한 실행 결과 확인을 위한 코드입니다.\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "\n",
        "# MNIST 데이터 세트를 불러옵니다.\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "# MNIST 데이터 세트를 Train set과 Test set으로 나누어 줍니다.\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()    \n",
        "\n",
        "# Train 데이터 5000개와 Test 데이터 1000개를 사용합니다.\n",
        "train_images, train_labels = train_images[:5000], train_labels[:5000]\n",
        "test_images, test_labels = test_images[:1000], test_labels[:1000]\n",
        "\n",
        "# CNN 모델의 입력으로 사용할 수 있도록 (샘플개수, 가로픽셀, 세로픽셀, 1) 형태로 변환합니다.\n",
        "train_images = tf.expand_dims(train_images, -1)\n",
        "test_images = tf.expand_dims(test_images, -1)\n",
        "\n",
        "\n",
        "# CNN 모델을 설정합니다.\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME', input_shape = (28,28,1)),\n",
        "    tf.keras.layers.MaxPool2D(padding = 'SAME'),\n",
        "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'),\n",
        "    tf.keras.layers.MaxPool2D(padding = 'SAME'),\n",
        "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'),\n",
        "    tf.keras.layers.MaxPool2D(padding = 'SAME'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "])\n",
        "\n",
        "# CNN 모델 구조를 출력합니다.\n",
        "print(model.summary())\n",
        "\n",
        "# CNN 모델의 학습 방법을 설정합니다.\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])\n",
        "              \n",
        "# 학습을 수행합니다. \n",
        "history = model.fit(train_images, train_labels, epochs = 10, batch_size = 128, verbose = 2)\n",
        "\n",
        "Visulaize([('CNN', history)], 'loss')\n",
        "\n",
        "\"\"\"\n",
        "1. 평가용 데이터를 활용하여 모델을 평가합니다.\n",
        "   loss와 accuracy를 계산하고 loss, test_acc에 저장합니다.\n",
        "\"\"\"\n",
        "loss, test_acc = model.evaluate(test_images, test_labels, verbose = 0)\n",
        "\n",
        "\"\"\"\n",
        "2. 평가용 데이터에 대한 예측 결과를 predictions에 저장합니다.\n",
        "\"\"\"\n",
        "predictions = model.predict_classes(test_images)\n",
        "\n",
        "# 모델 평가 및 예측 결과를 출력합니다.\n",
        "print('\\nTest Loss : {:.4f} | Test Accuracy : {}'.format(loss, test_acc))\n",
        "print('예측한 Test Data 클래스 : ',predictions[:10])\n",
        "\n",
        "# 평가용 데이터에 대한 레이어 결과를 시각화합니다.\n",
        "Plotter(test_images, model)\n"
      ],
      "metadata": {
        "id": "GgfJqkZDYGwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 영화 리뷰 긍정/부정 분류 RNN 모델 - 데이터 전 처리"
      ],
      "metadata": {
        "id": "ENA_giyrYVeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import data_process\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "import logging, os\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "# 학습용 및 평가용 데이터를 불러오고 샘플 문장을 출력합니다.\n",
        "X_train, y_train, X_test, y_test = data_process.imdb_data_load()\n",
        "\n",
        "\"\"\"\n",
        "1. 인덱스로 변환된 X_train, X_test 시퀀스에 패딩을 수행하고 각각 X_train, X_test에 저장합니다.\n",
        "   시퀀스 최대 길이는 300으로 설정합니다.\n",
        "\"\"\"\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=300, padding='post')\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=300, padding='post')\n",
        "\n",
        "print(\"\\n패딩을 추가한 첫 번째 X_train 데이터 샘플 토큰 인덱스 sequence: \\n\",X_train[0])"
      ],
      "metadata": {
        "id": "zWasJQHYZiva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 영화 리뷰 긍정/부정 분류 RNN 모델 - 모델 학습\n",
        "\n",
        "Keras에서 RNN 모델을 만들기 위해 필요한 함수/라이브러리\n",
        "\n",
        "일반적으로 RNN 모델은 입력층으로 Embedding 레이어를 먼저 쌓고, RNN 레이어를 몇 개 쌓은 다음, 이후 Dense 레이어를 더 쌓아 완성합니다.\n",
        "\n",
        "임베딩 레이어\n",
        "tf.keras.layers.Embedding(input_dim, output_dim, input_length)\n",
        "Copy\n",
        ": 들어온 문장을 단어 임베딩(embedding)하는 레이어\n",
        "\n",
        "input_dim: 들어올 단어의 개수\n",
        "\n",
        "output_dim: 결과로 나올 임베딩 벡터의 크기(차원)\n",
        "\n",
        "input_length: 들어오는 단어 벡터의 크기\n",
        "\n",
        "RNN 레이어\n",
        "\n",
        "tf.keras.layers.SimpleRNN(units)\n",
        "Copy\n",
        ": 단순 RNN 레이어\n",
        "\n",
        "units: 레이어의 노드 수"
      ],
      "metadata": {
        "id": "Vx9kEM3jZzGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import data_process\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "import logging, os\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "# 동일한 실행 결과 확인을 위한 코드입니다.\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "# 학습용 및 평가용 데이터를 불러오고 샘플 문장을 출력합니다.\n",
        "X_train, y_train, X_test, y_test = data_process.imdb_data_load()\n",
        "\n",
        "max_review_length = 300\n",
        "\n",
        "# 패딩을 수행합니다.\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length, padding='post')\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, padding='post')\n",
        "\n",
        "\n",
        "embedding_vector_length = 32\n",
        "\n",
        "\"\"\"\n",
        "1. 모델을 구현합니다.\n",
        "   임베딩 레이어 다음으로 `SimpleRNN`을 사용하여 RNN 레이어를 쌓고 노드의 개수는 5개로 설정합니다. \n",
        "   Dense 레이어는 0, 1 분류이기에 노드를 1개로 하고 activation을 'sigmoid'로 설정되어 있습니다.\n",
        "\"\"\"\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(1000, embedding_vector_length, input_length = max_review_length),\n",
        "    tf.keras.layers.SimpleRNN(5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "# 모델을 확인합니다.\n",
        "print(model.summary())\n",
        "\n",
        "# 학습 방법을 설정합니다.\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "# 학습을 수행합니다.\n",
        "model_history = model.fit(X_train, y_train, epochs = 3, verbose = 2)"
      ],
      "metadata": {
        "id": "Src5qmkgZ047"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 영화 리뷰 긍정/부정 분류 RNN 모델 - 평가 및 예측\n",
        "\n",
        "Keras에서 RNN 모델의 평가 및 예측을 위해 필요한 함수/메서드\n",
        "\n",
        "평가 방법\n",
        "\n",
        "model.evaluate(X, Y)\n",
        "Copy\n",
        "evaluate() 메서드는 학습된 모델을 바탕으로 입력한 feature 데이터 X와 label Y의 loss 값과 metrics 값을 출력합니다.\n",
        "\n",
        "예측 방법\n",
        "\n",
        "model.predict(X)\n",
        "Copy\n",
        "X 데이터의 예측 label 값을 출력합니다."
      ],
      "metadata": {
        "id": "M_zBjdC5aB7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import data_process\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "import logging, os\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "# 동일한 실행 결과 확인을 위한 코드입니다.\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "# 학습용 및 평가용 데이터를 불러오고 샘플 문장을 출력합니다.\n",
        "X_train, y_train, X_test, y_test = data_process.imdb_data_load()\n",
        "\n",
        "max_review_length = 300\n",
        "\n",
        "# 패딩을 수행합니다.\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length, padding='post')\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, padding='post')\n",
        "\n",
        "\n",
        "embedding_vector_length = 32\n",
        "\n",
        "\n",
        "# 모델을 구현합니다.\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(1000, embedding_vector_length, input_length = max_review_length),\n",
        "    tf.keras.layers.SimpleRNN(5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "# 모델을 확인합니다.\n",
        "print(model.summary())\n",
        "\n",
        "# 학습 방법을 설정합니다.\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "# 학습을 수행합니다.\n",
        "model_history = model.fit(X_train, y_train, epochs = 5, verbose = 2)\n",
        "\n",
        "\"\"\"\n",
        "1. 평가용 데이터를 활용하여 모델을 평가합니다.\n",
        "   loss와 accuracy를 계산하고 loss, test_acc에 저장합니다.\n",
        "\"\"\"\n",
        "loss, test_acc = model.evaluate(X_test, y_test, verbose = 0)\n",
        "\n",
        "\"\"\"\n",
        "2. 평가용 데이터에 대한 예측 결과를 predictions에 저장합니다.\n",
        "\"\"\"\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# 모델 평가 및 예측 결과를 출력합니다.\n",
        "print('\\nTest Loss : {:.4f} | Test Accuracy : {}'.format(loss, test_acc))\n",
        "print('예측한 Test Data 클래스 : ',1 if predictions[0]>=0.5 else 0)\n"
      ],
      "metadata": {
        "id": "qNvRVlqoapyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## "
      ],
      "metadata": {
        "id": "gAqbFVMOa10u"
      }
    }
  ]
}